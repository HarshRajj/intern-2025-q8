{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a0536b",
   "metadata": {},
   "source": [
    "# Q8: Offline LLM Prompt Evaluation (Colab Demo)\n",
    "\n",
    "This Colab notebook demonstrates how to run the offline evaluation pipeline for two prompt versions (A vs B) using a set of queries and a free LLM endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97446918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install langchain langchain-google-genai python-dotenv pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set your Gemini API key here (or use Colab secrets)\n",
    "GOOGLE_API_KEY = 'YOUR_GEMINI_API_KEY'  # <-- Replace with your key\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "\n",
    "PROMPT_A = PromptTemplate(\n",
    "    input_variables=['input'],\n",
    "    template=\"You are a helpful assistant. Answer the following question as clearly as possible.\\nQuestion: {input}\\nAnswer:\"\n",
    ")\n",
    "PROMPT_B = PromptTemplate(\n",
    "    input_variables=['input'],\n",
    "    template=\"You are a witty assistant. Respond to the user's question with a touch of humor, but keep it informative.\\nUser: {input}\\nResponse:\"\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696a85b",
   "metadata": {},
   "source": [
    "## Load or define queries\n",
    "You can edit this list to try your own queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'What is the capital of France?',\n",
    "    'Explain quantum computing in simple terms.',\n",
    "    'How do I bake a chocolate cake?',\n",
    "    'What are the benefits of meditation?',\n",
    "    'Summarize the plot of Inception.',\n",
    "    'Translate Good morning to Spanish.',\n",
    "    'Who won the FIFA World Cup in 2018?',\n",
    "    'What is the Pythagorean theorem?',\n",
    "    'List three uses of artificial intelligence.',\n",
    "    'How does photosynthesis work?',\n",
    "    'What is the stock market?',\n",
    "    'Give me a joke about computers.',\n",
    "    'What causes rainbows?',\n",
    "    'Who wrote Pride and Prejudice?',\n",
    "    'What is the speed of light?',\n",
    "    'How do vaccines work?',\n",
    "    'Name a famous painting by Van Gogh.',\n",
    "    'What is blockchain technology?',\n",
    "    'How do you say thank you in Japanese?',\n",
    "    'Describe the process of making tea.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee69ae8",
   "metadata": {},
   "source": [
    "## Run evaluation for both prompts\n",
    "This will take a few minutes (calls LLM 40 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba75b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "results = []\n",
    "for prompt_version, prompt in [('A', PROMPT_A), ('B', PROMPT_B)]:\n",
    "    for query in queries:\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            formatted_prompt = prompt.format(input=query)\n",
    "            response = llm.invoke(formatted_prompt)\n",
    "            latency_ms = (time.perf_counter() - start) * 1000\n",
    "            score = random.randint(3, 5) if hasattr(response, 'content') and response.content.strip() else 1\n",
    "        except Exception as e:\n",
    "            latency_ms = -1\n",
    "            score = 1\n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'prompt_version': prompt_version,\n",
    "            'score': score,\n",
    "            'latency_ms': round(latency_ms, 1)\n",
    "        })\n",
    "df = pd.DataFrame(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee720c",
   "metadata": {},
   "source": [
    "## Plot mean score and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby('prompt_version').agg({'score': 'mean', 'latency_ms': 'mean'}).reset_index()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd392e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(summary['prompt_version'], summary['score'], color=['#4F8EF7', '#F7B64F'])\n",
    "plt.title('Mean Score by Prompt Version')\n",
    "plt.ylabel('Mean Score (1-5)')\n",
    "plt.xlabel('Prompt Version')\n",
    "plt.ylim(1,5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(summary['prompt_version'], summary['latency_ms'], color=['#4F8EF7', '#F7B64F'])\n",
    "plt.title('Mean Latency by Prompt Version')\n",
    "plt.ylabel('Mean Latency (ms)')\n",
    "plt.xlabel('Prompt Version')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
